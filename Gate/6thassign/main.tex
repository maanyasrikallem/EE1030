\let\negmedspace\undefined
\let\negthickspace\undefined
\documentclass[journal]{IEEEtran}
\usepackage[a5paper, margin=10mm, onecolumn]{geometry}
%\usepackage{lmodern} % Ensure lmodern is loaded for pdflatex
\usepackage{tfrupee} % Include tfrupee package

\setlength{\headheight}{1cm} % Set the height of the header box
\setlength{\headsep}{0mm}     % Set the distance between the header box and the top of the text

\usepackage{gvv-book}
\usepackage{gvv}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts,amsthm}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{txfonts}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{gensymb}
\usepackage{comment}
\usepackage[breaklinks=true]{hyperref}
\usepackage{tkz-euclide} 
\usepackage{listings}
% \usepackage{gvv}                                        
\def\inputGnumericTable{}                                 
\usepackage[latin1]{inputenc}                                
\usepackage{color}                                            
\usepackage{array}                                            
\usepackage{longtable}                                       
\usepackage{calc}                                             
\usepackage{multirow}                                         
\usepackage{hhline}                                           
\usepackage{ifthen}                                           
\usepackage{lscape}
\begin{document}

\bibliographystyle{IEEEtran}
\vspace{3cm}

\title{ST-$2023$-$14$ to $26$}
\author{AI24BTECH11017 - MAANYA SRI
}
% \maketitle
% \newpage
% \bigskip
{\let\newpage\relax\maketitle}

\renewcommand{\thefigure}{\theenumi}
\renewcommand{\thetable}{\theenumi}
\setlength{\intextsep}{10pt} % Space between text and floats


\numberwithin{equation}{enumi}
\numberwithin{figure}{enumi}
\renewcommand{\thetable}{\theenumi}
\begin{enumerate}
\item Consider the probability space $ \brak{\Omega, \mathcal{G}, P}$, where $ \Omega = \sbrak{0, 2}$ and $\mathcal{G} = \cbrak{ \emptyset, \Omega, \sbrak{0, 1}, (1, 2]}$. Let $X$ and $Y$ be two functions on $\Omega$ defined as 
\begin{align*}
    X(\omega) = \begin{cases} 
      1 & if \omega \in \sbrak{0, 1} \\ 
      2 & if \omega \in (1, 2] 
   \end{cases}
\end{align*}
and 
\begin{align*}
    Y(\omega) = \begin{cases} 
      2 & if \omega \in \sbrak{0, 1.5} \\ 
      3 & if \omega \in (1.5, 2]. 
   \end{cases}
\end{align*}
Then which one of the following statements is true?
\begin{enumerate}
    \item $X$ is a random variable with respect to $ \mathcal{G}$, but $Y$ is not a random variable with respect to $ \mathcal{G}$
    \item $Y$ is a random variable with respect to $ \mathcal{G} $, but $X$ is not a random variable with respect to $ \mathcal{G}$
    \item Neither $X$ nor $Y$ is a random variable with respect to $ \mathcal{G} $
    \item Both $X$ and $Y$ are random variables with respect to $\mathcal{G} $
\end{enumerate}

\item Let $ \Phi(\cdot) $ denote the cumulative distribution function of a standard normal random variable. If the random variable $X$ has the cumulative distribution function 
\begin{align*}
F(x) = \begin{cases} 
      \Phi\brak{x} & if x < -1 \\ 
      \Phi\brak{x + 1} & if x \geq -1, 
   \end{cases}
\end{align*}
then which one of the following statements is true?
\begin{enumerate}
    \item $ P\brak{X \leq -1} = \frac{1}{2} $
    \item $ P\brak{X = -1} = \frac{1}{2} $
    \item $ P\brak{X < -1} = \frac{1}{2} $
    \item $ P\brak{X \leq 0} = \frac{1}{2} $
\end{enumerate}

\item Let $ X $ be a random variable with probability density function
\begin{align*}
    f(x) = \begin{cases} 
      \alpha \lambda x^{\alpha -1} e^{-\lambda x^{\alpha}} & if x > 0 \\ 
      0 & otherwise, 
   \end{cases}
\end{align*}
where $ \alpha > 0 $ and $ \lambda > 0 $. If the median of $ X $ is 1 and the third quantile is 2, then $ \brak{\alpha, \lambda} $ equals
\begin{enumerate}
    \item $\brak{1,  \log_e 2 }$
    \item $\brak{1, 1}$
    \item $\brak{2,  \log_e 2 }$
    \item $\brak{1,  \log_e 3 }$
\end{enumerate}

\item Let $ X $ be a random variable having Poisson distribution with mean $ \lambda > 0 $. Then
$E \brak{\cond {\frac{1}{X+1}} {X > 0}}$ equals
\begin{enumerate}
    \item $ \frac{1 - e^{-\lambda} - \lambda e^{-\lambda}}{\lambda \brak{1- e^{-\lambda}}} $
    \item $ \frac{1-e^{-\lambda}}{\lambda} $
    \item $ \frac{1 - e^{-\lambda} - \lambda e^{-\lambda}}{\lambda } $
    \item $ \frac{1-e^{-\lambda}}{\lambda+1} $
\end{enumerate}

\item Suppose that $ X $ has the probability density function
\begin{align*}
    f(x) = \begin{cases} 
      \frac{\lambda^{\alpha}}{\Gamma(\alpha)} x^{\alpha - 1} e^{-\lambda x} & if  x > 0 \\ 
      0 & otherwise, 
   \end{cases}
\end{align*}
where $ \alpha > 0 $ and $ \lambda > 0 $. Which one of the following statements is NOT true?
\begin{enumerate}
    \item $ E\brak{X}$ exists for all $ \alpha > 0 $ and $ \lambda > 0 $
    \item Variance of $ X $ exists for all $ \alpha > 0 $ and $ \lambda > 0 $
    \item $ E\brak{\frac{1}{X} } $ exists for all $ \alpha > 0 $ and $ \lambda > 0 $
    \item $ E\brak{\log_e (1 + X)}$ exists for all $ \alpha > 0 $ and $ \lambda > 0 $
\end{enumerate}

\item Let $ (X,Y) $ have joint probability density function
\begin{align*}
    f(x, y) = \begin{cases} 
      8xy & if 0 < x < y < 1 \\ 
      0 & otherwise. 
   \end{cases}
\end{align*}
If $ E\brak{\cond {X} {Y} = y_0}= \frac{1}{2} $, then $ y_0 $ equals
\begin{enumerate}
    \item $ \frac{3}{4} $
    \item $ \frac{1}{2} $
    \item $ \frac{1}{3} $
    \item $ \frac{2}{3} $
\end{enumerate}

\item Suppose that there are 5 boxes, each containing 3 blue pens, 1 red pen and 2 black pens. One pen is drawn at random from each of these 5 boxes. If the random variable $ X_1$ denotes the total number of blue pens drawn and the random variable $ X_2 $ denotes the total number of red pens drawn, then $ P\brak{X_1 = 2, X_2 = 1} $ equals
\begin{enumerate}
    \item $ \frac{5}{36} $
    \item $ \frac{5}{18} $
    \item $ \frac{5}{12} $
    \item $ \frac{5}{9} $
\end{enumerate}

\item Let $ \cbrak{X_n}_{n \geq 1} $ and $ \cbrak{Y_n}_{n \geq 1} $ be two sequences of random variables and $ X $ and $ Y $ be two random variables, all of them defined on the same probability space. Which one of the following statements is true?
\begin{enumerate}
    \item If $\cbrak{X_n}_{n \geq 1}$ converges in distribution to a real constant $ c $, then $ \cbrak{X_n}_{n \geq 1} $ converges in probability to $ c $
    \item If $ \cbrak{X_n}_{n \geq 1} $ converges in probability to $ X $, then $ \cbrak{X_n}_{n \geq 1} $ converges in $3^{rd}$ mean to $ X $
    \item If $ \cbrak{X_n}_{n \geq 1} $ converges in distribution to $ X $ and $ \cbrak{Y_n}_{n \geq 1} $ converges in distribution to $ Y $, then $ \cbrak{X_n + Y_n}_{n \geq 1} $ converges in distribution to $ X + Y $
    \item If $\cbrak{E\brak{X_n}}_{n \geq 1}$ converges to $E(X) $, then $ \cbrak{X_n}_{n \geq 1}$ converges in $1^{st}$ mean to $ X $
\end{enumerate}

\item Let $X$ be a random variable with probability density function
\begin{align*}
f\brak{x; \lambda} = \begin{cases} 
      \frac{1}{\lambda} e^{-\frac{x}{\lambda} }& if x > 0\\ 
      0 & otherwise, 
   \end{cases}
\end{align*}
where $\lambda > 0$ is an unknown parameter. Let $Y_1, Y_2, \ldots, Y_n$ be a random sample of size $n$ from a population having the same distribution as $X^2$.\\
If $\bar{Y} = \frac{1}{n} \sum_{i=1}^{n} Y_i$, then which one of the following statements is true?
\begin{enumerate}
    \item $\sqrt{\frac{\bar{Y}}{2}}$ is a method of moments estimator of $\lambda$
    \item $\sqrt{\bar{Y}}$ is a method of moments estimator of $\lambda$
    \item $\frac{1}{2} \sqrt{\bar{Y}}$ is a method of moments estimator of $\lambda$
    \item $2 \sqrt{\bar{Y}}$ is a method of moments estimator of $\lambda$
\end{enumerate}

\item Let $X_1, X_2, \ldots, X_n$ be a random sample of size $n \brak{\geq 2}$ from a population having probability density function
\begin{align*}
f(x; \theta) = \begin{cases} 
      \frac{2}{\theta x} \brak{-log_e x} e^{-\frac{\brak{log_e x}^2}{\theta}}  & if 0 < x < 1 \\ 
      0 & otherwise, 
   \end{cases}
\end{align*}
where $\theta > 0$ is an unknown parameter. Then which one of the following statements is true?

\begin{enumerate}
    \item $\frac{1}{n}\sum_{i=1}^n \brak{log_e X_i}^2$ is the maximum likelihood estimator of $\theta$
    \item $\frac{1}{n-1}\sum_{i=1}^n \brak{log_e X_i}^2$ is the maximum likelihood estimator of $\theta$
    \item $\frac{1}{n} \sum_{i=1}^n log_e X_i$ is the maximum likelihood estimator of $\theta$
    \item $\frac{1}{n-1} \sum_{i=1}^n log_e X_i$ is the maximum likelihood estimator of $\theta$
\end{enumerate}

\item Let $X_1, X_2, \ldots, X_n$ be a random sample of size $n$ from a population having uniform distribution over the interval $\brak{\frac{1}{3},\theta}$ ,where $\theta > \frac{1}{3}$ is an unknown parameter. If $Y = max\cbrak{X_1,X_2,...,X_n}$, then which one of the following statements is true?

\begin{enumerate}
    \item $\brak{\frac{n+1}{n}}\brak{Y-\frac{1}{3}} + \frac{1}{3}$ is an unbiased estimator of $\theta$
    \item $\brak{\frac{n}{n+1}}\brak{Y-\frac{1}{3}} + \frac{1}{3}$ is an unbiased estimator of $\theta$
    \item $\brak{\frac{n+1}{n}}\brak{Y+\frac{1}{3}} - \frac{1}{3}$ is an unbiased estimator of $\theta$
    \item $Y$ is an unbiased estimator of $\theta$
\end{enumerate}

\item Suppose that $X_1, X_2, \ldots, X_n,Y_1, Y_2, \ldots, Y_n$ are independent and identically distributed random vectors each having $N_p(\mu, \Sigma)$ distribution, where $\Sigma$ is non-singular, $p > 1$ and $n>1$. If $\bar{X} = \frac{1}{n} \sum_{i=1}^{n} X_i$ and $\bar{Y} = \frac{1}{n} \sum_{i=1}^{n} Y_i$, then which one of the following statements is true?

\begin{enumerate}
    \item There exists $c > 0$ such that $c (\bar{X} - \mu)^T \Sigma^{-1} (\bar{X} - \mu)$ has $\chi^2$-distribution with $p$ degrees of freedom
    \item There exists a $c > 0$ such that $c (\bar{X} - \bar{Y})^T \Sigma^{-1} (\bar{X} - \bar{Y})$ has $\chi^2$-distribution with $\brak{p-1}$ degrees of freedom.
    \item There exists $c > 0$ such that $c \sum_{i=1}^{n} \brak{X_i - \bar{X}}^T \Sigma^{-1} \brak{X_i - \bar{X}} $ has $\chi^2$-distribution with $p$ degrees of freedom
    \item There exists $c > 0$ such that $c \sum_{i=1}^{n} \brak{X_i - Y_i - \bar{X}+\bar{Y}}^T \Sigma^{-1}  \brak{X_i - Y_i - \bar{X}+\bar{Y}}$ has $\chi^2$-distribution with $p$ degrees of freedom
\end{enumerate}

\item Consider the following regression model
\begin{align*}
y_k &= \alpha_0 + \alpha_1 \log_e k + \epsilon_k, \quad k = 1, 2, \dots, n,
\end{align*}
where the $ \epsilon_k $'s are independent and identically distributed random variables each having probability density function $f(x) = \frac{1}{2}e^{-|x|}, \quad x \in \mathbb{R}$.
Then which of the following statements is true?

\begin{enumerate}
    \item The maximum likelihood estimator of $ \alpha_0 $ does not exist
    \item The maximum likelihood estimator of $ \alpha_1 $ does not exist
    \item The least squares estimator of $ \alpha_0 $ exists and is unique
    \item The least squares estimator of $ \alpha_1 $ exists, but it is not unique
\end{enumerate}


 \end{enumerate}
 \end{document}

